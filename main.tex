\documentclass[sigconf,review]{acmart}

\title[GPT vs CG]{GPT vs CG: Evaluating the performance of GPT-4 on Computer Graphics assessment questions}

% \author{Haoran Feng}
% \email{hfen962@aucklanduni.ac.nz}
% \affiliation{
%   \institution{University of Auckland}
%   \city{Auckland}
%   \country{New Zealand}
% }

% \author{Paul Denny}
% \email{p.denny@auckland.ac.nz}
% \affiliation{
%   \institution{University of Auckland}
%   \city{Auckland}
%   \country{New Zealand}
% }

% \author{Burkhard C. W\"{u}nsche}
% \email{burkhard@cs.auckland.ac.nz}
% \affiliation{
%   \institution{University of Auckland}
%   \city{Auckland}
%   \country{New Zealand}
% }

% \author{Andrew Luxton-Reilly}
% \email{a.luxton-reilly@auckland.ac.nz}
% \affiliation{
%   \institution{University of Auckland}
%   \city{Auckland}
%   \country{New Zealand}
% }

% \author{Steffan Hooper}
% \email{shoo635@aucklanduni.ac.nz}
% \affiliation{
%   \institution{University of Auckland}
%   \city{Auckland}
%   \country{New Zealand}
% }

\author{Anonymous Author1}
\email{anon1@university.edu}
\affiliation{
  \institution{Anon Institution}
  \city{Anon City}
  \country{Anon Country}
}

\author{Anonymous Author2}
\email{anon2@university.edu}
\affiliation{
  \institution{Anon Institution}
  \city{Anon City}
  \country{Anon Country}
}

\author{Anonymous Author3}
\email{anon3@university.edu}
\affiliation{
  \institution{Anon Institution}
  \city{Anon City}
  \country{Anon Country}
}

\author{Anonymous Author4}
\email{anon4@university.edu}
\affiliation{
  \institution{Anon Institution}
  \city{Anon City}
  \country{Anon Country}
}

\author{Anonymous Author5}
\email{anon5@university.edu}
\affiliation{
  \institution{Anon Institution}
  \city{Anon City}
  \country{Anon Country}
}

\renewcommand{\shortauthors}{Authors, et al.}

\copyrightyear{2024}
\acmYear{2024}
\setcopyright{rightsretained}
\acmConference[SIGCSE 2024]{Proceedings of the 2024 Technical Symposium on Computer Science Education (SIGCSE TS)}{March 20--23, 2024}{Portland, Oregon}
\acmBooktitle{Proceedings of the 2024 Technical Symposium on Computer Science Education (SIGCSE TS), March 20--23, 2024, Portland, Oregon}\acmDOI{xxxxxxxxxxx}
\acmISBN{xxx-x-xxxx-xxxx-x/xx/xx}


\begin{document}

\begin{abstract}
Recent studies have showcased the exceptional performance of LLMs (Large Language Models) on assessment questions across multiple subjects, such as Law, Medicine, and Computer Science. This makes it possible for students to generate solutions quickly and easily for their assessed work without engaging in the learning process. One countermeasure for instructors is to create questions for which LLMs perform poorly, but little is known about the characteristics of such questions, especially in upper-level courses such as CG (Computer Graphics).  Assessment questions in CG tend to cover a wide variety of concepts and question types. To address this gap, we evaluated the performance of GPT-4 on past assessment questions used in a final-year undergraduate course about introductory CG. We found that its performance was slightly below that of an average student and it struggled to achieve a passing rate.

We classified assessment questions and evaluated the performance of GPT-4 for different types of questions and found that performance tended to be best for simple mathematical questions, and worst for questions with complex descriptions and requiring creative thinking to derive a solution, especially those with images. Our research provides new insights into the capabilities of GPT-4 and opportunities for teaching staff to alleviate academic dishonest behaviour from using LLMs.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010179.10010182</concept_id>
       <concept_desc>Computing methodologies~Natural language generation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010371</concept_id>
       <concept_desc>Computing methodologies~Computer graphics</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010405.10010489</concept_id>
       <concept_desc>Applied computing~Education</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Natural language generation}
\ccsdesc[500]{Computing methodologies~Computer graphics}
\ccsdesc[500]{Applied computing~Education}

\keywords{Artificial Intelligence, Large Language Models, GPT-4, Computer Graphics, Evaluation, Assessment, Academic Integrity}

\maketitle

\section{Introduction}
Recent studies have showcased the exceptional performance of LLMs (Large Language Models) on assessment questions across several fields of study, achieving, or even exceeding, the performance of an average student \cite{testcodex, testcopilot, testgpt, testcs2, testhigher, testbar, testmed}. This highlights the success of LLMs. However, there are also caveats to the popularisation of LLMs, such as the more frequent occurrences of academic dishonesty \cite{acadint}. Since ChatGPT \cite{chatgpt}, an online text generation tool, is free for public use, students can easily conduct academic dishonesty by generating solutions to assignments, or even exams, to pass courses \cite{gptacadint}. This disadvantages honest students that complete their work independently and limits the learning of dishonest students. Hence countermeasures must be taken to ensure the fairness of assessments following the popularisation of ChatGPT and other LLM-based tools.

Invigilation makes it harder to use LLMs but is resource intensive and usually only done for selected assessments, such as final exams.
%% and even then invigilators will s and cannot always be enforced, such as for assignments. 
Another approach is using questions that ChatGPT, or other LLM-based tools, perform poorly on. While past research has evaluated the performance of LLMs in several subjects \cite{testcodex, testcopilot, testgpt, testcs2, testhigher, testbar, testmed}, most of these subjects are relatively homogeneous, e.g., programming questions for introductory computing and essay-style questions in law and medicine. More research is necessary to understand LLMs' capabilities to answer assessment questions in a diverse range of subjects. Also, it is recommended to evaluate and monitor the performance of LLMs continuously since a recent study  concluded that the performance of ChatGPT on some assessment questions has decreased over the past few months \cite{gptworse}.

Several research projects have evaluated the performance of LLMs on introductory programming questions \cite{testcodex, testcopilot, testgpt}. However, for many other disciplines in Computer Science, the performance of LLMs has not yet been reported. In our research, we want to investigate how well LLMs perform for computing subjects combining knowledge from a diverse range of disciplines. We decided to use CG (Computer Graphics), since solving CG problems requires diverse skills such as programming skills, understanding of APIs, different forms of reasoning, translational skills, creative skills, and mathematical skills \cite{Suselo2017,Rodrigues2021}. Furthermore, the field is interesting since information can be represented by text, code, formulas, and images~\cite{Suselo2022a,Suselo2022b}.  We are unaware of any previous work evaluating LLMs for such a heterogeneous subject. We will analyse how LLMs perform for different types of questions, using different types of input and output, and requiring different solution strategies. 
%%From the results of the evaluation, we aim to connect different question categories with the different performance outcomes, devise possible explanations and theories for the observed data, and suggest characteristics of questions on which LLMs perform poorly in the general scope. 
We decided to use GPT-4 as the LLM to be investigated since it, and its predecessors, have been used in most previous research in this field.

We aim to answer the following research questions:
\begin{enumerate}
    \item[\textbf{RQ1:}] Can GPT-4 achieve human performance on assessment questions used in an introductory Computer Graphics course?
    \item[\textbf{RQ2:}] Does GPT-4 perform differently on different question categories? If so, what question categories does GPT-4 perform well/poorly on?
\end{enumerate}

\section{Related Work}
Much research has been conducted to evaluate the capability of LLMs in question-answering. For example, as stated in the original paper of GPT-3 \cite{gpt3}, the model achieved an impressive accuracy of 64.3\% on TriviaQA \cite{triviaqa}, a large-scale reading comprehension and question-answering dataset. Katz et al. evaluated the performance of GPT-4 on the Uniform Bar Examination. The model achieved significantly higher than the minimum passing threshold, furthermore, it achieved higher than the average human candidate in five of seven subjects \cite{testbar}. Liévin et al. evaluated the performance of a model based on GPT-3.5 on three medical question-answering datasets, in which the model achieved human-level performances on all three datasets \cite{testmed}.

In the field of Computer Science, multiple similar studies have been conducted. Finnie-Ansley et al. evaluated the performance of Codex, a code generation LLM based on GPT-3, on assessment questions used in a CS1 introductory Python programming course, and the model scored 78.5\% and 78.0\% on two separate tests, placing itself on the top quartile within the class performance \cite{testcodex}. Denny et al. evaluated the performance of GitHub Copilot (based on Codex) on another set of CS1 introductory Python programming problems, and Copilot was capable of solving more than half of the problems on the first try, while solving an even higher proportion of problems with further prompt engineering \cite{testcopilot}. Savelka et al. investigated the performance of text-davinci-003 (one of the most advanced GPT models from OpenAI) on assessments in introductory and intermediate Python programming courses, and the model achieved scores of around 60\% for all three courses \cite{testgpt}. LLMs also achieved high performance on CS2 programming exercises \cite{testcs2} and programming-related multiple-choice questions \cite{testhigher}.

In contrast, Singla showed that ChatGPT and GPT-4 performed poorly for visual programming questions, possibly because the models lack spatial skills required for visual programming \cite{testvisual}.

% \begin{figure*}
%     \centering
%     \includegraphics[]{cube1.png}
%     \;
%     \includegraphics[]{cube2.png}
%     \;
%     \includegraphics[]{light.png}
%     \caption{Example images used in assessments. The ``novice'' image description for the left image is ``The image shows a RGB colour cube with side length 2. The colours shown are green, cyan, blue, pink, red, and yellow in counterclockwise order, where red is on the x-axis.''; the ``informed'' image description for the left image is ``The image shows a RGB colour cube with side length 2. The point representing white is on (2, 2, 2), the point representing blue is on (0, 0, 2), and the point representing red is on (2, 0, 0).''. The ``novice'' image description for the middle image is ``The image shows a RGB colour cube with side length 2 shifted one unit to the right. The colours shown are red, pink, white, cyan, green, and black in counterclockwise order, where green is on the x-axis.''; the ``informed'' image description for the middle image is ``The image shows a RGB colour cube with side length 2 shifted one unit to the positive direction of the x-axis. The point representing white is on (1, 0, 2), the point representing blue is on (3, 2, 2), and the point representing red is on (1, 2, 0).''. The ``novice'' image description for the right image is ``A sphere is shown surrounded by darkness.''; the ``informed'' image description for the right image is ``A sphere is shown in space. The space surrounding it is black, the sphere itself is shown with a medium level of brightness, and there are no highlights shown on the sphere.''.}
%     \label{Images}
% \end{figure*}

\begin{table*}
    \centering
    \caption{Example images used in assessments along with their image descriptions}
    \label{Images}
    \begin{tabular}{|c|p{5.8cm}|p{5.8cm}|}
        \hline
        \multicolumn{1}{|c|}{Image} & \multicolumn{1}{|c|}{Novice Description} & \multicolumn{1}{|c|}{Informed Description} \\ \hline\hline
        % \raisebox{-0.94\totalheight}{\includegraphics[]{cube1.png}} & The image shows a RGB colour cube with side length 2. The colours shown are green, cyan, blue, pink, red, and yellow in counterclockwise order, where red is on the x-axis. & The image shows a RGB colour cube with side length 2. The point representing white is on (2, 2, 2), the point representing blue is on (0, 0, 2), and the point representing red is on (2, 0, 0). \\ \hline
        \raisebox{-0.94\totalheight}{\includegraphics[]{house.jpg}} & A shape of a house is drawn in the xy-plane. The house is made up of a square with a triangle on top. & A shape of a house is drawn in the xy-plane. The house is made up of a square with a triangle on top. The square is made up of four points, (0, 0), (2, 0), (2, 2), and (0, 2). The triangle is made up of three points, (0, 2), (2, 2), and (1, 3). \\ \hline
        \raisebox{-0.94\totalheight}{\includegraphics[]{cube2.png}} & The image shows a RGB colour cube with side length 2 shifted one unit to the right. The colours shown are red, pink, white, cyan, green, and black in counterclockwise order, where green is on the x-axis. & The image shows a RGB colour cube with side length 2 shifted one unit to the positive direction of the x-axis. The point representing white is on (1, 0, 2), the point representing blue is on (3, 2, 2), and the point representing red is on (1, 2, 0). \\ \hline
        \raisebox{-0.94\totalheight}{\includegraphics[]{light.png}} & A sphere is shown surrounded by darkness. & A sphere is shown in space. The space surrounding it is black, the sphere itself is shown with a medium level of brightness, and there are no highlights shown on the sphere. \\ \hline
    \end{tabular}
\end{table*}

\begin{table}
    \centering
    \caption{The percentages of correct generated solutions by criteria}
    \label{Percentage}
    \begin{tabular}{|l|r|}
    \hline
        \multicolumn{1}{|p{6cm}|}{Criterion} & Percentage \\ \hline
        \hline
        All questions & 42.06\% \\ \hline
        \hline
        Multiple-choice questions & 53.51\% \\ \hline
        Programming (1 attempt)& 27.14\% \\ \hline
        Programming (10 attempts) & 53.57\% \\ \hline
        \hline
        Contains no images & 60.00\% \\ \hline
        Contains images & 32.21\% \\ \hline
        Contains images (with novice descriptions) & 27.94\% \\ \hline
        Contains images (with informed descriptions) & 36.47\% \\ \hline
        \hline
        Text only & 52.86\% \\ \hline
        Text and mathematical formulas only & 64.81\% \\ \hline
        \hline
        Deductive reasoning & 45.91\% \\ \hline
        Inductive reasoning & 35.00\% \\ \hline
%%    \end{tabular}
%%    \begin{tabular}{|l|r|}
%%        \hline
%%        \multicolumn{1}{|p{6cm}|}{Criterion} & Percentage \\ \hline
        \hline
        Easy difficulty & 56.00\% \\ \hline
        Medium difficulty & 51.33\% \\ \hline
        Hard difficulty & 18.16\% \\ \hline
        \hline
        Topic: 3D Modelling & 75.00\% \\ \hline
        Topic: Texture Mapping & 60.00\% \\ \hline
        Topic: Geometry & 50.00\% \\ \hline
        Topic: Colour & 46.67\% \\ \hline
        Topic: Parametric Curves and Surfaces & 44.55\% \\ \hline
        Topic: Illumination and Shading & 43.57\% \\ \hline
        Topic: Ray Tracing & 36.43\% \\ \hline
        Topic: Graphics Introduction & 30.00\% \\ \hline
        Topic: Image Processing & 25.62\% \\ \hline
        % Topic: Geometry & 50.00\% \\ \hline
        % Topic: Graphics Introduction & 30.00\% \\ \hline
        % Topic: Colour & 46.67\% \\ \hline
        % Topic: Illumination and Shading & 43.57\% \\ \hline
        % Topic: 3D Modelling & 75.00\% \\ \hline
        % Topic: Texture Mapping & 60.00\% \\ \hline
        % Topic: Ray Tracing & 36.43\% \\ \hline
        % Topic: Parametric Curves and Surfaces & 44.55\% \\ \hline
        % Topic: Image Processing & 25.62\% \\ \hline
    \end{tabular}
\end{table}

% \begin{figure*}
%     \centering
%     \begin{tabular}{|l|r|r|r|r|}
%         \hline
%         \multicolumn{1}{|p{1.2cm}|}{Novice} & \multicolumn{1}{c|}{2022 Test} & \multicolumn{1}{c|}{2022 Exam} & \multicolumn{1}{c|}{2023 Test} & \multicolumn{1}{c|}{2023 Exam} \\ \hline\hline
%         Run 0 & 36.36\% & 35.00\% & 68.75\% & 23.31\% \\ \hline
%         Run 1 & 50.00\% & 29.17\% & 31.25\% & 32.63\% \\ \hline
%         Run 2 & 59.09\% & 35.83\% & 47.92\% & 29.24\% \\ \hline
%         Run 3 & 40.91\% & 20.83\% & 56.25\% & 31.78\% \\ \hline
%         Run 4 & 45.45\% & 34.17\% & 45.83\% & 39.41\% \\ \hline
%         Run 5 & 36.36\% & 20.83\% & 62.50\% & 34.32\% \\ \hline
%         Run 6 & 50.00\% & 26.67\% & 45.83\% & 37.29\% \\ \hline
%         Run 7 & 40.91\% & 35.83\% & 66.67\% & 32.63\% \\ \hline
%         Run 8 & 36.36\% & 29.17\% & 52.08\% & 32.63\% \\ \hline
%         Run 9 & 50.00\% & 35.83\% & 68.75\% & 38.14\% \\ \hline\hline
%         Mean & 44.54\% & 30.33\% & 54.58\% & 33.14\% \\ \hline
%         Std & 7.67\% & 6.00\% & 12.30\% & 4.68\% \\ \hline
%     \end{tabular}
%     \;
%     \begin{tabular}{|l|r|r|r|r|}
%         \hline
%         \multicolumn{1}{|p{1.2cm}|}{Informed} & \multicolumn{1}{c|}{2022 Test} & \multicolumn{1}{c|}{2022 Exam} & \multicolumn{1}{c|}{2023 Test} & \multicolumn{1}{c|}{2023 Exam} \\ \hline\hline
%         Run 0 & 50.00\% & 46.67\% & 62.50\% & 25.00\% \\ \hline
%         Run 1 & 54.55\% & 25.83\% & 60.42\% & 30.93\% \\ \hline
%         Run 2 & 54.55\% & 37.50\% & 43.75\% & 38.56\% \\ \hline
%         Run 3 & 45.45\% & 34.17\% & 50.00\% & 41.10\% \\ \hline
%         Run 4 & 50.00\% & 40.83\% & 62.50\% & 39.41\% \\ \hline
%         Run 5 & 45.45\% & 34.17\% & 68.75\% & 34.32\% \\ \hline
%         Run 6 & 45.45\% & 35.00\% & 52.08\% & 40.68\% \\ \hline
%         Run 7 & 36.36\% & 34.17\% & 60.42\% & 30.93\% \\ \hline
%         Run 8 & 40.91\% & 34.17\% & 58.33\% & 37.71\% \\ \hline
%         Run 9 & 54.55\% & 32.50\% & 62.50\% & 38.14\% \\ \hline\hline
%         Mean & 47.73\% & 35.50\% & 58.13\% & 35.68\% \\ \hline
%         Std & 6.16\% & 5.45\% & 7.38\% & 5.25\% \\ \hline
%     \end{tabular}
%     \caption{The scores achieved on assessments using novice image descriptions (left) and informed image descriptions (right)}
%     \label{Score}
% \end{figure*}

\begin{table}
    \centering
    \caption{The expected scores of assessments for both the Novice and Informed GPT prompts, compared with the mean scores of past students}
    \label{Score}
    \begin{tabular}{|l|r|r|r|r|}
        \hline

                \multicolumn{1}{|p{1.2cm}|}{} & \multicolumn{1}{c|}{2022} & \multicolumn{1}{c|}{2022} & \multicolumn{1}{c|}{2023} & \multicolumn{1}{c|}{2023} \\
                
        \multicolumn{1}{|p{1.2cm}|}{} & \multicolumn{1}{c|}{Test} & \multicolumn{1}{c|}{Exam} & \multicolumn{1}{c|}{Test} & \multicolumn{1}{c|}{Exam} \\ \hline\hline
        Novice GPT & 63.64\% & 48.83\% & {71.88\%} & 41.69\% \\ \hline
        Informed GPT & 67.73\% & 53.83\% & {78.12\%} & {47.03\%} \\ \hline
        Student & {77.55\%} & {69.33\%} & 64.00\% & {46.43\%} \\ \hline
    \end{tabular}
    % \begin{tabular}{|l|r|r|r|r|}
    %     \hline
    %     \multicolumn{1}{|p{1.2cm}|}{Informed} & \multicolumn{1}{c|}{2022 Test} & \multicolumn{1}{c|}{2022 Exam} & \multicolumn{1}{c|}{2023 Test} & \multicolumn{1}{c|}{2023 Exam} \\ \hline\hline
    %     Informed GPT & 67.73\% & 53.83\% & \textbf{78.12\%} & \textbf{47.03\%} \\ \hline
    %     Student & \textbf{77.55\%} & \textbf{69.33\%} & 64.00\% & 46.43\% \\ \hline
    % \end{tabular}
\end{table}

\section{Methodology}
To evaluate the performance of GPT models on CG assessment questions, 136 questions were collected from computerised assessments used in an undergraduate course specialising in CG but which also included the topics ``Colour'', ``Geometry'', and basic ``Image Processing''. The questions were preprocessed and fed as input to GPT-4 through OpenAI's ChatCompletion API (temperature: 0.75). The assessments were approximately 50\% multiple-choice questions and 50\% programming questions. 10 responses were generated for each question, and the generated solutions were manually reviewed and classified into meaningful categories. The data was then processed and discussed.

Most questions could not be inputted directly into GPT-4, as they contained formulas and images, which were not supported as input types, hence they needed to be replaced with textual alternatives that carry similar information.

Two formula preprocessing methods were considered: textual replacement and TeX command replacement. Replacing each formula with its corresponding textual counterpart was the most straightforward approach. However, formulas can be written differently, e.g., the formula $x^2$ may be written as ``x\^{}2'', ``x*x'', and ``x²''. This can potentially lead to disparities in results, as GPT-4 may generate different tokens from the two strings. To maximise consistency, each formula was replaced with its TeX command counterpart, as it is a more rigorous method of representing formulas, and there is usually only one TeX command to represent each formula. Additionally, if students are to use GPT models to ``aid'' their problem-solving process, then they would most likely copy and paste the TeX command directly from the assessment page, instead of typing the formula manually.

Several image preprocessing methods were explored: description generation, image-input GPT models, and manual description. Online tools that generate descriptions based on an image upload were explored, but much crucial information relevant to the questions was usually not captured in the generated descriptions. For example, numbers presented in images were usually overlooked in the descriptions, and they were necessary for solving the question, hence this method was considered unsuitable. Alternatively, there exist similar ChatCompletion GPT models that accept images as input, such as MiniGPT-4 \cite{mini}. However, these models also tended to fail at distinguishing vital information for solving the questions, and their answers did not indicate a thorough understanding of the questions. This behaviour was unsurprising as the models were not designed and trained for this purpose, hence this method was also rejected. Although time-consuming and cumbersome, manual image description seemed to be the only viable method to represent images in this scenario. From the perspective of students, image descriptions may also be the best method to replace images in questions, as it is the most accessible and straightforward.

\label{imageslimitation}
A problem with replacing images with descriptions is that they are highly subjective, i.e., people have different understandings and interpretations of images, and thus give different descriptions. To provide more insight into the capabilities and variability of GPT-4, we tested it using two different descriptions for each image. The objective was to achieve results similar to those two different students would obtain when sitting the assessments with the assistance of only GPT-4.

The first description of each image mimics that of a novice student with a very limited understanding of CG. These descriptions include basic observations, but lack important details, and understanding of relevant concepts and terminologies. 

The second description mimics that of an informed student with a good understanding of CG. These descriptions include more observations, describe relevant concepts, use the correct terminology, and mention details important for finding a solution. Examples of image descriptions are shown in Table \ref{Images}.

After generating the solutions, we manually evaluated the performance by classifying the generated solutions based on their correctness. We further classified each question into a variety of categories, e.g., by considering assessed topics, question types, formats of questions (text, image, formulas), reasoning process, etc.

\section{Results}

We evaluated all 136 questions of two mid-term tests and two final exams using GPT-4 and image descriptions mimicking ``Novice'' and ``Informed'' students.
Table \ref{Percentage} shows the percentages of correctly generated answers for different types of questions using 10 generations. For questions using images, we used 10 generations each using novice and informed descriptions. The left column shows the criterion used to describe the type of question, e.g., all programming questions or questions without images. The right column shows what percentage of generated answers for that group of questions was correct.

Table \ref{Score} shows the total mark GPT-4 would have achieved for the four tests and exams using image descriptions mimicking novice students and those mimicking informed students. The bottom row shows the average mark for all students in each assessment. the total mark reflects the fact that different questions have in general different marks, e.g., more difficult questions usually have higher marks.
For multiple-choice questions, we used the average score out of 10 generations to reflect the fact that students have only one attempt for each answer. 
For programming questions, we used the maximum score out of 10 generations to reflect the fact that students had unlimited attempts and the automated assessment system used provided immediate feedback on the obtained score \cite{codeRunner1}.


% Table \ref{Score} shows the scores achieved on different past assessments, where the left table shows the results using novice image descriptions for image-based questions, and the right shows the results using informed image descriptions. The differences in scores in the two tables are only caused by the different image descriptions in image-based questions, as the generated solutions used for other questions are exactly the same. Each ``Run X'' row shows the results using a different set of generated solutions, and the ``Mean'' and ``Std'' rows show the average and standard deviation of the achieved scores across all 10 sets of generations. The ``Mean'' row approximates the scores a novice/informed student would achieve on the past assessments by using generated solutions only.

\section{Discussion}

Our results illustrate that the performance of GPT-4 can vary considerably between question types and exams. Here we discuss these differences in more detail and suggest explanations and implications.

\subsection{Overall Results}

Table \ref{Percentage} shows the GPT-4 answered 42.06\% of questions correctly. Since for programming questions, we used the best results out of 10 generations, the performance of GPT-4 is better than that when looking at the total marks of an assessment displayed in Table \ref{Score}. GPT-4 performs better in the tests than the exams and worst in the 2023 exam. This corresponds with the assessment of the teaching staff who considered the tests easier and the exams harder with the 2023 exam hardest. Overall, GPT-4 performed worse than an average student and failed to pass any of the exams apart from the 2022 exam using informed image descriptions.

Note that student performance in the 2022 assessments was considerably higher than for the 2023 assessments. We believe this is due to the fact that the 2023 assessments were invigilated and conducted in computer labs, whereas the 2022 assessments were not invigilated and done at home. The performance of GPT-4 more closely resembles the teaching staffs' assessment of difficulty and sudden variations between students' and GPT-4's performance might hence be an indicator of widespread academic dishonesty.


\subsection{Programming Questions}

Out of all solutions generated for programming questions, 27.14\% contained the exact code needed to gain full marks for the respective question (successful compilation and all test cases passed). Since the assessment conditions used allowed unlimited attempts with no penalties, we computed the proportion of questions with at least one correct generated solution out of the 10, which was 53.57\%.

Out of the set of incorrectly generated solutions for programming questions, 20.10\% only contained minor errors that can be fixed with small modifications. They usually occurred when generated code did not follow specified rules. Some examples are listed below.
\begin{itemize}
    \item Function call notation: Functions were called with incorrect syntax, e.g., ``foo.func(bar)'' was used instead of the correct ``func(foo, bar)''.
    \item Operand order: Some questions specified the order of operands for some operations, and the order was not followed, e.g., ``Integer * Vector'' was used instead of the specified ``Vector * Integer''.
    \item Spelling: Rare minor spelling errors were found in function calls, e.g., ``normalized()'' was used instead of the specified ``normalised()''.
\end{itemize}

Most of the other incorrectly generated solutions involved algorithmic and/or logic errors and required more manual effort to amend. However, all generated solutions appeared plausible, and it was difficult to detect the error(s) causing incorrect results.

\subsection{Image-based Questions}

GPT-4 performed much worse on questions containing images (32.21\%) than those containing none (60.00\%). An essential skill taught in CG is the ability to interpret visual and spatial information \cite{visual} and such skills are required to perform well \cite{Liu2022}. GPT-4 performed better for informed image descriptions (36.47\%) than novice image descriptions (27.94\%), since the former were more detailed and contained all relevant information to solve a question. We were surprised that the differences were not larger. We suggest that the reasoning required to combine the textual descriptions of visual information with task descriptions is either too complex or insufficiently considered in the training data for GPT-4.


% The results in Table \ref{Score} also show that the level of understanding demonstrated in image descriptions leads to a difference in resultant scores. A two-tailed paired t-test was carried out between the scores in the two tables, where each score in the left table is paired with the corresponding score in the right table, and the resultant p-value shows that there is a significant difference (p-value: 0.004138), i.e. GPT-4 performed higher for questions with informed image descriptions than those with novice image descriptions. This shows that the insight and the extra information provided by the user positively influence the performance of GPT-4. This could be due to the fact that while the images were being described, the visual processing required to solve the question was partially completed by the human interpreter, hence less visual processing (or perhaps none) was necessary for GPT-4 to perform, and it was capable to obtain more correct answers relying only on its textual processing abilities.

\subsection{Mathematical Questions}

Mathematics is crucial in CG, as most concepts in CG involve calculations and geometry \cite{Suselo2017,Balreira2018}. Out of the total 136 questions, 84 questions (61.76\%) include mathematical formulas or notations. Basic concepts in linear algebra, such as vectors and dot products, are prominent in these questions. GPT-4 performed the highest in questions involving mathematical formulas (64.81\%) among all observed criteria, most notably surpassing purely text-based questions.

A possible reason for this is that steps in solving mathematical questions follow a strict, rigorous procedure, whereas semantics needs to be taken into account when solving text-based questions, hence the mathematical, rigorous reasoning process may be captured more easily in LLMs than the semantic reasoning process used in text-based questions.

Another possible reason is that the mathematical concepts tested in CG assessments are fundamental and also used in other disciplines. For example, linear algebra is used in engineering, physics, and economics. Hence CG exam questions about mathematical concepts are more general and related material is available on the Internet, making it more likely that GPT-4 has learned relevant information \cite{gpt3}. In contrast, text-based questions are often more specific, such as finding the most suitable surface representation for a given modelling task. 

Although GPT-4 generated correct answers for the majority of questions, some generated answers failed to follow straightforward procedures. Below are some errors that GPT-4 made when generating answers for mathematical questions.
\begin{itemize}
    \item Calculation error: we found instances of missing negatives or incorrect one-digit addition/subtraction operations.
    \item Incorrect formula substitution: numbers were substituted in incorrect places when applying formulas.
    \item Logic error: we found illogical statements in solutions, e.g., unequal terms were equated.
\end{itemize}

\subsection{Reasoning and Difficulty}

GPT-4 performed better for questions classified as ``deductive reasoning'', at 45.91\%, than those classified as ``inductive reasoning'', at 35.00\%. This is in line with recent results reported in the literature~\cite{Espejel2023,liu2023}.

Deductive reasoning makes inferences by going from general premises to specific conclusions, e.g., if all cubic splice curves have a polynomial degree of 3, what is the degree of the basis functions of the Hermite curve? Inductive reasoning makes chain inferences by going from specifics to general statements, e.g., from shadow properties inferring light source properties. The assessment questions that use deductive reasoning mostly involved applying formulas, recalling knowledge and concepts, and logical thinking, whereas those that use inductive reasoning often required creative thinking that does not follow a rigorous procedure.
GPT-4 seems to have higher logical thinking capabilities than creative thinking capabilities, and is better at following a rigorous procedure than devising a procedure independently.

We would like to note that questions requiring inductive and creative thinking are also perceived as difficult by staff and students. For example, from the questions that were classified by teaching staff as ``Hard'', 71.05\% used inductive reasoning. As shown in Table \ref{Score}, GPT-4 performed worse for difficult questions than other questions, and hence the low performance for questions using inductive reasoning might be caused by such questions being harder.

The three most difficult questions of the 2023 exam had less than 5\% of students answering them correctly. All of these questions required inductive reasoning and GPT-4 couldn't answer any of them. An example is writing code computing the normal of a cut sphere. This required inferring that any surface point not fulfilling the sphere equation must be on the cutting surface and hence the surface normal at that point is that of the cutting plane.

\subsection{Topics}

GPT-4 performed worst in ``Image Processing'', ``Graphics Introduction'', and ``Ray Tracing''. These topics are heavily associated with images and other forms of visualisation, and most of the questions included images or pixel matrices representing images. We theorise that GPT-4 lacks visual and spatial processing capabilities.

GPT-4 performed best in``3D Modelling'', ``Texture Mapping'', and ``Geometry''. There were only 4 questions associated with the topic ``3D Modelling'', where the 3 questions answered correctly were multiple-choice questions mostly based on text, hence the high performance could be due to chance. Questions on the topic ``Geometry'' included many mathematical questions, which GPT-4 achieved relatively high performance on, and that possibly led to the relatively high overall performance on the topic. The high performance for ``Texture Mapping'' was surprising, as the relevant questions rely heavily on visual representations. A possible reason is that images and/or image descriptions contained numbers to describe vertex positions and texture coordinates and based on this it seems GPT-4 was able to infer missing values and/or code to perform these operations.

GPT-4 performed surprisingly poorly on the topic ``Colour''. All questions were pure text questions without images about colour theory. For example, a simple question, which most students were able to answer but GPT-4 couldn't, related to the perceived colour of an object with a given material colour and illuminated with a coloured light source. The answer can be derived both by reasoning and applying formulas, but was outside the capabilities of GPT-4.

\subsection{Assessment Marks}

Table \ref{Score} suggests that there is a correlation between students' total marks in an assessment and the total marks achieved by GPT-4 using novice or informed image descriptions. We did not compute the correlation mathematically since there is too much noise in the data, e.g., 2022 assessments were not invigilated.

While on average GPT-4 performed worse than the average student, we notice that GPT-4 using informed image descriptions performed slightly better than the average student for the 2023 invigilated assessments. However, informed image descriptions require insight into what information is relevant for GPT-4 to answer questions.

Without human help, GPT-4 does not (yet) achieve the performance of an average student, but our results show that it can pass the (generally easier) tests and can almost pass exams. Hence, it is a viable tool for students to cheat in CG assessments and the existence of GPT-4 (and other LLMs) poses a significant threat to academic honesty and relevant countermeasures need to be devised.

% Although GPT-4 did not consistently reach average student performance, it showed capabilities to pass both the 2022 and 2023 iterations of the course. In other words, a student is likely to pass the assessment component of the course (achieving 50\% from test and exam combined) regardless of their level of knowledge, and this highlights the danger of not addressing academic dishonest behaviour and devising relevant countermeasures.

\subsection{Additional Observations}

We made several additional observations not fitting into the previous subsections, which might be of interest to readers.

\noindent \textit{\textbf{Convincing solutions}}: All generated solutions were convincing at first glance, regardless of the correctness of the solution. If there existed errors in the solution, then they were usually subtle, and meticulous inspections were often required to identify the errors.

\noindent \textit{\textbf{No explanations}}: Sometimes, GPT-4 did not generate explanations or justifications along with the answer. In many instances, the generated solutions for multiple-choice questions were solely the text provided as one of the options.

\noindent \textit{\textbf{Contradictory statements}}: Some generated solutions contained contradictory statements. For example, one solution claimed that (a) is the correct answer at the start, then claimed that (b) is the correct answer at the end.

\noindent \textit{\textbf{Hallucinations}}: For a few multiple-choice questions, some answers were generated with the wrong corresponding letter. For example, one solution included the text from option (a), but the chosen option was (b).

\noindent \textit{\textbf{Falsely confident}}: Although many generated solutions contained a variety of different texts, plenty of questions yielded identical generations that were all incorrect.

\subsection{Suggestions}
From the findings of this study, we suggest countermeasures, possibly suitable for disciplines other than CG, that may alleviate academic dishonest behaviour from the usage of LLMs.

\noindent \textit{\textbf{Include specific requirements for programming questions}}: We have found that generated solutions for programming questions often do not adhere to specific requirements. Hence, LLMs may perform worse for programming questions that have specific requirements, such as a certain syntax, custom functions, and unique naming conventions. However, we are wary that this might disadvantage some students, e.g., those who have English as a second language or reading difficulties (e.g., dyslexia).

\noindent \textit{\textbf{Use more image-based questions}}: Our results suggest that using images in question descriptions makes it harder to cheat. In such cases, the performance of GPT-4 depends on the quality of the image description and students with poor understanding of the field are unlikely to describe images well enough to enable GPT-4 to generate correct answers.
Students with a good understanding of the field will get more correct answers when using GPT-4 (reflecting their better understanding) but are still unlikely to get enough correct answers to pass.
%%From the results related to image-based questions, we proposed that, with only the assistance of LLMs, students with more knowledge of the topic are more likely to score higher on image-based questions than those less knowledgeable, while still scoring significantly lower than the average student, thus image-based questions can assess student ability to an extent even with the availability of LLMs, and the incorporation of more image-based questions may be a viable method of fairly assessing students.

\noindent \textit{\textbf{Less deductive reasoning and more inductive reasoning}}: GPT-4 performed better for questions using deductive reasoning, such as mathematical questions, than those using inductive reasoning, such as those requiring creative thinking. Thus to control the performance of GPT-4, and other LLMs, on future assessment questions, questions that require students to recall knowledge, apply formulas, and form simple deductions should be avoided, and questions that require longer thought processes and more intensive creative thinking should be used instead.

However, we note that in our experience many students struggle with questions using inductive reasoning. So, while reducing cheating, such questions might also prevent borderline to average students to pass.

\subsection{Limitations}

Our study has several limitations that we acknowledge below.

\noindent \textit{\textbf{Subjective image descriptions}}: As addressed briefly in Section \ref{imageslimitation}, a major limitation of this study is the fact that we did require image descriptions and the format and quality will depend on individual users. Since the performance of GPT-4 for such questions depends on the image descriptions, different results might be obtained when using different descriptions. 

\noindent \textit{\textbf{Subjective classifications}}: Our classification of questions into easy, medium, and hard was done by teaching staff and is subjective. In the past, students usually performed poorly in hard questions, but occasionally also struggled with easy-to-medium questions. Usually, these were short questions requiring a key idea or making themselves an illustration (which students are encouraged to do, but scrap paper analysis shows that few students do it). An example is computing the distance between two balls, which is the distance between the centres of the balls minus both radii.

\noindent \textit{\textbf{Sequential inputs for programming questions}}: In this study, each solution was generated independently, i.e., one generation does not affect another. However, the possibility of sequential inputs was not explored. The use of sequential prompts may boost the performance \cite{wei2023}. Errors or discrepancies between the output and the expected output may be reported back to the LLM, and the solution may be adjusted accordingly, where each iteration brings the generation closer to a correct solution. Unfortunately, this was outside the scope of this research. 

\noindent \textit{\textbf{Untested suggestions}}: Although the suggestions devised are supported by data, there is no experimental evidence that they can reduce academic dishonesty. Further experiments, possibly across other disciplines, need to be conducted to evaluate the effectiveness of these suggestions.

\section{Conclusion and Future Work}

In conclusion, we evaluated the performance of GPT-4 on CG assessment questions and found that it could not consistently reach average student performance, but was able to pass 2 of the 4 assessments while reaching an astonishing 78.12\% for one of the assessments. GPT-4 showed great capabilities when solving CG assessments, and with LLMs being publicly available, countermeasures for academic dishonest behaviour from LLMs became a necessity. We classified the questions and generated solutions into categories, suggested commonalities of questions that GPT-4 performed well/poorly on, and devised several suggestions when writing future assessment questions for better robustness against cheating.

The field of LLMs is advancing and improving rapidly, and new tools are frequently being developed, hence the performances of LLMs are ever-changing. This study marks a milestone for the current performance of LLMs, but may become outdated in the near future, thus the performance of LLMs should be continuously monitored and evaluated. Moreover, GPT-4 accepting image inputs will most likely be released to the public in the near future \cite{gpt4images}, so the performance of LLMs on CG assessment questions, and those in countless other fields, will likely reach a new level.

In future work, we would like to compare the performance of GPT-4 and students for more assessments. In particular, from before we used automatic assessment tools and whether the performance of GPT-4 relative to students is changing over time. We would also like to explore how much better GPT-4 performs when using chain-of-thought prompting.

\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}

\end{document}